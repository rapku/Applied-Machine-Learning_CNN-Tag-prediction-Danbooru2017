{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rapha\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lda_danbooru.pkl', 'rb') as file:\n",
    "    LDA_danbooru = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training.pkl', 'rb') as file:\n",
    "    training_data = pickle.load(file)\n",
    "with open('test.pkl', 'rb') as file:\n",
    "    test_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = 'D:/UMSI Classes/Fall2018/670/Project/danbooru2017/metadata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on json file 2017000000000000.json\n",
      "0\n",
      "10000\n",
      "Running on json file 2017000000000001.json\n",
      "20000\n",
      "Running on json file 2017000000000002.json\n",
      "30000\n",
      "Running on json file 2017000000000003.json\n",
      "40000\n",
      "50000\n",
      "Running on json file 2017000000000004.json\n",
      "60000\n",
      "Running on json file 2017000000000005.json\n",
      "70000\n",
      "Running on json file 2017000000000006.json\n",
      "80000\n",
      "90000\n",
      "Running on json file 2017000000000007.json\n",
      "100000\n",
      "Running on json file 2017000000000008.json\n",
      "110000\n",
      "Running on json file 2017000000000009.json\n",
      "120000\n",
      "130000\n",
      "Running on json file 2017000000000010.json\n",
      "140000\n",
      "Running on json file 2017000000000011.json\n",
      "150000\n",
      "Running on json file 2017000000000012.json\n",
      "160000\n",
      "170000\n",
      "Running on json file 2017000000000013.json\n",
      "180000\n",
      "Running on json file 2017000000000014.json\n",
      "190000\n",
      "Running on json file 2017000000000015.json\n",
      "200000\n",
      "210000\n",
      "Running on json file 2017000000000016.json\n",
      "220000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Extract tags from SFW pictures - 226k pictures, should have the picture on file.\n",
    "Take only tags that have 0 category (general tags, not series, character, artist, or Danbooru meta)\n",
    "'''\n",
    "i = 0\n",
    "tags = {}\n",
    "\n",
    "list_of_jsons = os.listdir(metadata)\n",
    "\n",
    "for metadata_json in list_of_jsons:\n",
    "    json_path = os.path.join(metadata+metadata_json)\n",
    "    print('Running on json file {}'.format(metadata_json))\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        random.seed(453432)\n",
    "        for num, line in enumerate(file):\n",
    "            try:\n",
    "                json_set = json.loads(line)\n",
    "                #check if id exists and is safe for work\n",
    "\n",
    "                if int(json_set['id'])%1000 >= 100 or json_set['rating'] != 's':\n",
    "                    continue\n",
    "\n",
    "                img_id = json_set['id']\n",
    "\n",
    "                img_tags = [tag['name'] for tag in json_set['tags'] if tag['category'] == '0']\n",
    "\n",
    "                tags[img_id] = img_tags\n",
    "\n",
    "                if i%10000 == 0:\n",
    "                    print(i)\n",
    "\n",
    "                i+= 1\n",
    "\n",
    "            except Exception:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226484"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags = [taglist for taglist in tags.values()]\n",
    "\n",
    "len(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "tag_dict = corpora.Dictionary(all_tags)\n",
    "tag_dict.filter_extremes(no_below=5)\n",
    "\n",
    "bag_of_words = [tag_dict.doc2bow(taglist) for taglist in tags.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already loaded\n"
     ]
    }
   ],
   "source": [
    "if LDA_danbooru:\n",
    "    print('Already loaded')\n",
    "    pass\n",
    "else:\n",
    "    LDA_danbooru = LdaModel(bag_of_words, num_topics=10, id2word=tag_dict)\n",
    "    with open('lda_danbooru.pkl', 'wb') as file:\n",
    "        pickle.dump(LDA_danbooru, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_ids = [x[0] for x in training_data]\n",
    "training_data_tags = [tags[str(x)] for x in training_data_ids]\n",
    "\n",
    "test = tag_dict.doc2bow(training_data_tags[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.123928085),\n",
       " (1, 0.011117434),\n",
       " (2, 0.011111391),\n",
       " (3, 0.40599573),\n",
       " (4, 0.011113007),\n",
       " (5, 0.011112221),\n",
       " (6, 0.011112949),\n",
       " (7, 0.011111698),\n",
       " (8, 0.01111281),\n",
       " (9, 0.3922847)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_danbooru.get_document_topics(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_weights(img_id):\n",
    "    final_topics = [0]*10\n",
    "    img_id = str(img_id)\n",
    "    taglist = tags[img_id]\n",
    "    bow_taglist = tag_dict.doc2bow(taglist)\n",
    "    topics = LDA_danbooru.get_document_topics(bow_taglist)\n",
    "    for pair in topics:\n",
    "        final_topics[pair[0]] = pair[1]\n",
    "    \n",
    "    return final_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_LDA = [(x[0], x[1], get_doc_weights(x[0])) for x in training_data]\n",
    "test_data_LDA = [(x[0], x[1], get_doc_weights(x[0])) for x in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_LDA.pkl', 'wb') as file:\n",
    "    pickle.dump(training_data_LDA, file)\n",
    "with open('test_LDA.pkl', 'wb') as file:\n",
    "    pickle.dump(test_data_LDA, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
